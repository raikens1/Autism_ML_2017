{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting ASD diagnosis from Genetic Data\n",
    "\n",
    "0/1 classification with logistic regression is a well-studied problem.  In order to familiarize myself with standard logistic regression techniques, I'm going to start with the simple two-class classification problem of predicting ASD/non-ASD diagnosis from genotype.\n",
    "\n",
    "Author: Rachael Caelie \"Rocky\" Aikens\n",
    "\n",
    "Created: Oct 25, 2017\n",
    "\n",
    "Version: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import f1_score, make_scorer, roc_curve\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We have genotype information for siblings from the Agre and Simons Simplex Collection, which has been featurized into a binary matrix (described below). In addition to that, we have imputed ASD/non-ASD labels and ADOS/ADI-R scores for a subset of those individuals.\n",
    "\n",
    "### Feature Data (Genotype)\n",
    "\n",
    "The input data is represented as a binary matrix.  There are a couple different representations we can use here, but to start I will use 1 = loss of function variant(compound het or homozygous alternate), 0 = no loss of function variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load input feature dataset for Agre\n",
    "Agre_asd = pd.read_csv(\"../../../iHART/kpaskov/CGT/data/v34_lof_asd_af0.50.txt\", index_col=0).transpose()\n",
    "Agre_ctrl = pd.read_csv(\"../../../iHART/kpaskov/CGT/data/v34_lof_typical_af0.50.txt\", index_col=0).transpose()\n",
    "\n",
    "print \"Cases: \", Agre_asd.shape[0]\n",
    "print \"Controls: \", Agre_ctrl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load input feature dataset for SSC\n",
    "SSC_asd = pd.read_csv(\"../../../iHART/kpaskov/CGT/data/SSC_lof_asd_af0.50.txt\", index_col=0).transpose()\n",
    "SSC_ctrl = pd.read_csv(\"../../../iHART/kpaskov/CGT/data/SSC_lof_typical_af0.50.txt\", index_col=0).transpose()\n",
    "\n",
    "print \"Cases: \", SSC_asd.shape[0]\n",
    "print \"Controls: \", SSC_ctrl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge SSC and Agre data\n",
    "X_asd = pd.concat([SSC_asd, Agre_asd], axis = 0).fillna(0)\n",
    "X_ctrl = pd.concat([SSC_ctrl, Agre_ctrl], axis = 0).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X_asd, X_ctrl], axis=0)\n",
    "print \"Total cases: \", X_asd.shape[0]\n",
    "print \"Total controls: \", X_ctrl.shape[0]\n",
    "print \"Features (ie. genes): \", X.shape[1]\n",
    "print \"Missing Values: \", int(X.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Data (ASD/non-ASD diagnosis)\n",
    "\n",
    "We have a file that Kelley has made with inferred Autism/Control diagnosis for the individuals in the iHart study.  We will try and predict diagnosis 0 = Control, 1 = Austism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"../../../iHART/kpaskov/PhenotypeGLRM/data/all_samples_filtered_labels.csv\", usecols = ['identifier','diagnosis'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shift y to a 0/1 representation for Control/ASD\n",
    "y[\"diagnosis\"] = np.where(y['diagnosis'] == 'Autism', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for Overlap\n",
    "\n",
    "Our phenotype labels y may not perfectly overlap with our genotype data, X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get lists of individuals in X and Y\n",
    "m_x = X.index.values.tolist()\n",
    "m_x_asd = X_asd.index.tolist()\n",
    "m_x_ctrl = X_ctrl.index.tolist()\n",
    "m_y = y.index.values.tolist()\n",
    "\n",
    "# check subject overlap between X and Y\n",
    "print \"%d subjects in X are not in y.  Of these, %d are cases and %d are controls.\" % (len(set(m_x) - set(m_y)), len(set(m_x_asd) - set(m_y)), len(set(m_x_ctrl) - set(m_y)))\n",
    "\n",
    "# make a list of Subject IDs with overlap\n",
    "subjects = list(set(m_x) & set(m_y))\n",
    "print \"This leaves %d subjects: %d cases and %d controls.\" % (len(subjects), len(set(m_x_asd) & set(m_y)), len(set(m_x_ctrl)&set(m_y))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The set of \"cases\" and \"controls\" appear to be differently defined between the iHart Phenotype labels (i.e. our `y` labels) and the CGT matrix labels (i.e. our `X` features). \n",
    "\n",
    "You can notice that the majority of controls don't appear in our phenotype information dataset. This is because ADOS\\ADI-R was not administered to many controls from SSC and Agre. Since we're interested in classifying ASD/non-ASD, for our purposes it is not necessary to exclude these individuals because we do not necessarily need any phenotype information outside of diagnosis. Rather, we can infer that all individuals in a 'control' CGT matrix without ADOS/ADI-R information have a non-ASD diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_add = list(set(m_x_ctrl) - set(m_y))\n",
    "y_ctrl = pd.DataFrame(np.zeros(len(to_add),), columns = ['diagnosis'],index = to_add)\n",
    "y = pd.concat([y, y_ctrl], axis = 0)\n",
    "subjects = subjects + to_add\n",
    "print len(subjects)\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# redefine X and Y to contain only the subjects we want\n",
    "X = X.ix[subjects]\n",
    "y = y.ix[subjects]\n",
    "\n",
    "# check we have the same subject IDs in the same order for X and Y\n",
    "print y.index.values.tolist() == X.index.values.tolist()\n",
    "y = y.ix[:,0]\n",
    "print y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that's probably going to be an issue for this experiment is that there are very few controls for whom we have both genetic and ADOS/ADI-R information.  This is going to mean that a random classifier performs with fairly high accuracy, just because classifying most or all individuals as autistic is a effective strategy when we have so few negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Since we have ~1,600 examples, I'm going to hold out 20% of the data as a test set and then do 5 fold cross validation using built-in sklearn methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(143)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Model and training parameters\n",
    "\n",
    "I am going to implement logistic regression using sklearn.\n",
    "\n",
    "We'll start with the following parameters:\n",
    "\n",
    "**Cost function**\n",
    "- Penalty distance metric = $L_2$\n",
    "- Dual formulation = `False` (better when $m$ > $n$)\n",
    "- c ($\\frac{1}{\\lambda}$ for regularization) = 1\n",
    "\n",
    "**Optimization Algorithm**\n",
    "- tolerance for convergence = $1\\times 10^{-4}$\n",
    "- optimization algorithm = liblinear\n",
    "\n",
    "**Model definition**\n",
    "- fit_intercept = `True`\n",
    "- class weighting = None\n",
    "- multi_class = 'ovr'\n",
    "\n",
    "More or less, these are the sklearn defaults, which I can tune at a later point.\n",
    "\n",
    "I've built a python object called EvalLR which will help me run cross validation for my regression models and output plots and statistics.  The following code initializes an EvalLR with the logistic regression model described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import EvalLR\n",
    "import class_EvalLR\n",
    "reload(class_EvalLR)\n",
    "from class_EvalLR import EvalLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-pass 5-fold cross validation\n",
    "\n",
    "To start, I'm going to run 5-fold cross validation on the training set. The following code does the necessary split for the data and prints the train and test scores for each fold using the f1 scoring metric.  Recall that this is:\n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{r} + \\frac{1}{p}} = \\frac{2rp}{r + p},$$\n",
    "\n",
    "where $r$ represents the *recall* or *sensitivity* of the classification and $p$ represents the *precision*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalr = EvalLR(X_train, y_train, reg = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores, topvals = evalr.kfold(7, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the training and testing F1 scores for each fold of cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores\n",
    "print \"Train:\", np.mean(scores.Train_score)\n",
    "print \"Test:\", np.mean(scores.Test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to training a predictive model, we'd also like to be able to make inference about which of the features in our CGT matrix are most informative for predicting Austism status.  The following are the top 10 genes with the greatest average odds ratios (i.e. farthest from 1). In order to make statistical inference, however, we should calculate a Wald statistic for each feature and search for the most significantly predictive features using multiple hypothesis testing correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print topvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Since our testing performance f1-scores are still about .1 below our training scores, it makes some sense to look into tuning our regularization parameter $C$ to avoid overfit.  I've written a function below, `reg_plot`, which performs 5 fold cross validation for models with different values of $C$.  \n",
    "\n",
    "**Note** Recall that $C$ is the inverse of the cannonical regularization parameter, $\\lambda$, so that smaller $C$ corresponds to stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a plot of preformance versus f1 score for different c values\n",
    "def reg_plot(c_vals, X_train, y_train, resample = False):\n",
    "    c_scores = []\n",
    "    print \"Running 7-fold cross validation for:\"\n",
    "    for i in range(len(c_vals)):\n",
    "        #print \"C = %f\" % c_vals[i]\n",
    "        evalr = EvalLR(X_train, y_train, reg = 'l2', c = c_vals[i])\n",
    "        c_scores.append(np.mean(evalr.kfold(7, False, False, resample)[0].Test_score))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.plot(c_vals, c_scores, linestyle = '-')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    return c_vals[c_scores.index(max(c_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_vals = [2**5, 2**4, 2**3, 2**2, 2, 2**-1, 2**-2, 2**-3, 2**-4, 2**-5]\n",
    "c_opt = reg_plot(c_vals, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first pass, it seems like lower regularization parameters may increase performance on the testing set.  Let's zoom in on performance for $C \\leq 1$ (since 1 is the default value we've previously been using):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_vals = [1, 2**-1, 2**-2, 2**-3, 2**-4, 2**-5, 2**-6, 2**-7, 2**-8, 2**-9, 2**-10]\n",
    "c_opt = reg_plot(c_vals, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that stronger regularization may improve performance.  Let's try again with a stronger regularization (the max of the plot above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalr = EvalLR(X_train, y_train, reg = 'l2', c = c_opt)\n",
    "scores, topgenes = evalr.kfold(7, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores\n",
    "print \"Train:\", np.mean(scores.Train_score)\n",
    "print \"Test:\", np.mean(scores.Test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print topgenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling data to address class imbalance\n",
    "\n",
    "Right now, we have a class imbalance problem because we tend to have many more autistic than non-autistic people in our dataset.  In general, we currently have two autistic subjects for every control subject in our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to address this problem is to resample from our data so that there is a 1:1 raio between austistic subjects and non-autistic controls.  In this implementation, I oversample the neurotypical subjects; that is, the positive subjects in the training data remain the same, but I sample with replacement from the neurotypical subjects in the training set for each fold so that there are as many controls as ASD cases in both the training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrain our model with resampled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalr = EvalLR(X_train, y_train, reg = 'l2')\n",
    "scores, topgenes = evalr.kfold(7, makeROC = True, resample = True); print scores; print topgenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resampling, the ROC curves look about the same.  The average training score is increased, and the average testing error is about the same as before resampling with this regularization parameter. \n",
    "\n",
    "We can tune the regularization parameter as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_vals = [2**6, 2**5, 2**4, 2**3, 2**2, 2, 2**-1, 2**-2, 2**-3, 2**-4, 2**-5]\n",
    "reg_plot(c_vals, X_train, y_train, resample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_vals = [1, 2**-1, 2**-2, 2**-3, 2**-4, 2**-5, 2**-6, 2**-7, 2**-8, 2**-9, 2**-10]\n",
    "c_opt = reg_plot(c_vals, X_train, y_train, resample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalr = EvalLR(X_train, y_train, reg = 'l2', c = c_opt)\n",
    "scores, topgenes = evalr.kfold(7, makeROC = True, resample = True); print scores; print topgenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "The following changes might be good to think about:\n",
    "- continue to improve EvalLR for fine-tuning regression models.\n",
    "- get wald statistic to detect important features\n",
    "- try other regression methods\n",
    "- is f1 score the best method when class imbalance is high?  Maybe I should be using AU-ROC or AU-PRC.\n",
    "- tune regularization parameter to try and reduce the variance we have\n",
    "- upsample to avoid class imbalance\n",
    "- split train/test data based on family ID just like sibkfold\n",
    "- add precision-recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
