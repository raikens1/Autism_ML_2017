{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting ASD diagnosis from Genetic Data\n",
    "\n",
    "Author: Rachael Caelie \"Rocky\" Aikens\n",
    "\n",
    "Created: Dec 7, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We have genotype information for siblings from the Agre and Simons Simplex Collection, which has been featurized into a binary matrix (described below). In addition to that, we have imputed ASD/non-ASD labels and ADOS/ADI-R scores for a subset of those individuals.\n",
    "\n",
    "### Feature Data (Genotype)\n",
    "\n",
    "The input data is represented as a binary matrix.  There are a couple different representations we can use here, but to start I will use 1 = loss of function variant(compound het or homozygous alternate), 0 = no loss of function variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load input feature dataset for Agre\n",
    "Agre_asd = pd.read_csv(\"../../../iHART/kpaskov/CGT/data/v34_lof_asd_af0.50.txt\", index_col=0).transpose()\n",
    "Agre_ctrl = pd.read_csv(\"../../../iHART/kpaskov/CGT/data/v34_lof_typical_af0.50.txt\", index_col=0).transpose()\n",
    "\n",
    "print \"Cases: \", Agre_asd.shape[0]\n",
    "print \"Controls: \", Agre_ctrl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load input feature dataset for SSC\n",
    "SSC_asd = pd.read_csv(\"../../../iHART/kpaskov/CGT/data/SSC_lof_asd_af0.50.txt\", index_col=0).transpose()\n",
    "SSC_ctrl = pd.read_csv(\"../../../iHART/kpaskov/CGT/data/SSC_lof_typical_af0.50.txt\", index_col=0).transpose()\n",
    "\n",
    "print \"Cases: \", SSC_asd.shape[0]\n",
    "print \"Controls: \", SSC_ctrl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge SSC and Agre data\n",
    "X_asd = pd.concat([SSC_asd, Agre_asd], axis = 0).fillna(0)\n",
    "X_ctrl = pd.concat([SSC_ctrl, Agre_ctrl], axis = 0).fillna(0)\n",
    "X = pd.concat([X_asd, X_ctrl], axis=0)\n",
    "print \"Total cases: \", X_asd.shape[0]\n",
    "print \"Total controls: \", X_ctrl.shape[0]\n",
    "print \"Features (ie. genes): \", X.shape[1]\n",
    "print \"Missing Values: \", int(X.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Data (ASD/non-ASD diagnosis)\n",
    "\n",
    "We have a file that Kelley has made with inferred Autism/Control diagnosis for the individuals in the iHart study.  We will try and predict diagnosis 0 = Control, 1 = Austism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"../../../iHART/kpaskov/PhenotypeGLRM/data/all_samples_filtered_labels.csv\", usecols = ['identifier','diagnosis'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shift y to a 0/1 representation for Control/ASD\n",
    "y[\"diagnosis\"] = np.where(y['diagnosis'] == 'Autism', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for Overlap\n",
    "\n",
    "Our phenotype labels y may not perfectly overlap with our genotype data, X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get lists of individuals in X and Y\n",
    "m_x = X.index.values.tolist()\n",
    "m_x_asd = X_asd.index.tolist()\n",
    "m_x_ctrl = X_ctrl.index.tolist()\n",
    "m_y = y.index.values.tolist()\n",
    "\n",
    "# check subject overlap between X and Y\n",
    "print \"%d subjects in X are not in y.  Of these, %d are cases and %d are controls.\" % (len(set(m_x) - set(m_y)), len(set(m_x_asd) - set(m_y)), len(set(m_x_ctrl) - set(m_y)))\n",
    "\n",
    "# make a list of Subject IDs with overlap\n",
    "subjects = list(set(m_x) & set(m_y))\n",
    "print \"This leaves %d subjects: %d cases and %d controls.\" % (len(subjects), len(set(m_x_asd) & set(m_y)), len(set(m_x_ctrl)&set(m_y))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The set of \"cases\" and \"controls\" appear to be differently defined between the iHart Phenotype labels (i.e. our `y` labels) and the CGT matrix labels (i.e. our `X` features). \n",
    "\n",
    "You can notice that the majority of controls don't appear in our phenotype information dataset. This is because ADOS\\ADI-R was not administered to many controls from SSC and Agre. Since we're interested in classifying ASD/non-ASD, for our purposes it is not necessary to exclude these individuals because we do not necessarily need any phenotype information outside of diagnosis. Rather, we can infer that all individuals in a 'control' CGT matrix without ADOS/ADI-R information have a non-ASD diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add in controls without explicit diagnosis\n",
    "to_add = list(set(m_x_ctrl) - set(m_y))\n",
    "y_ctrl = pd.DataFrame(np.zeros(len(to_add),), columns = ['diagnosis'],index = to_add)\n",
    "y = pd.concat([y, y_ctrl], axis = 0)\n",
    "subjects = subjects + to_add\n",
    "\n",
    "# redefine X and Y to contain only the subjects we want\n",
    "X = X.ix[subjects]\n",
    "y = y.ix[subjects]\n",
    "\n",
    "# check we have the same subject IDs in the same order for X and Y\n",
    "print y.index.values.tolist() == X.index.values.tolist()\n",
    "y = y.ix[:,0]\n",
    "print y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that's probably going to be an issue for this experiment is that there are very few controls for whom we have both genetic and ADOS/ADI-R information.  This is going to mean that a random classifier performs with fairly high accuracy, just because classifying most or all individuals as autistic is a effective strategy when we have so few negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "Since we have ~1,600 examples, I'm going to hold out 20% of the data as a test set and then do 5 fold cross validation using built-in sklearn methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(143)\n",
    "from class_SibKFold import SibKFold\n",
    "skf = SibKFold(5, X)\n",
    "train_ids, test_ids = skf.split(X)[0]\n",
    "X_train = X.ix[train_ids]\n",
    "X_test = X.ix[test_ids]\n",
    "y_train = y.ix[train_ids]\n",
    "y_test = y.ix[test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(train_ids)\n",
    "print len(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import EvalLR\n",
    "import class_EvalGBM\n",
    "reload(class_EvalGBM)\n",
    "from class_EvalGBM import EvalGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base classifier without resampling\n",
    "evalgbm = EvalGBM(X_train, y_train)\n",
    "scores = evalgbm.kfold(7, True)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(scores.Test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify again with resampling\n",
    "scores = evalgbm.kfold(7, True, resample = True)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(scores.Test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning parameters\n",
    "\n",
    "One possible parameter we can tune is `n_trees`, which determines the number of estimators to build. The code below measures model performance for a variety of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a plot of preformance versus f1 score for different c values\n",
    "def reg_plot(ntree_vals, X_train, y_train, resample = True):\n",
    "    ntree_scores = []\n",
    "    print \"Running 7-fold cross validation for:\"\n",
    "    for i in range(len(ntree_vals)):\n",
    "        print \"C = %f\" % ntree_vals[i]\n",
    "        evalgbm = EvalGBM(X_train, y_train, n_estimators = ntree_vals[i])\n",
    "        ntree_scores.append(np.mean(evalgbm.kfold(7, False, False, resample).Test_score))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.ylabel('mean cross validation f1 score')\n",
    "    plt.xlabel('Number of weak estimators')\n",
    "    plt.plot(ntree_vals, ntree_scores, linestyle = '-')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    return ntree_vals[ntree_scores.index(max(ntree_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntree_vals = [20,40, 50, 100, 150, 200, 250, 300, 350]\n",
    "reg_plot(ntree_vals, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that a smaller number of estimators may increase test performance, so let's retrain a model with 20 trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalgbm = EvalGBM(X_train, y_train, n_estimators = 20)\n",
    "scores = evalgbm.kfold(7, True, resample = True)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores\n",
    "print \"Train:\", np.mean(scores.Train_score)\n",
    "print \"Test:\", np.mean(scores.Test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good start, but this classifier type has a large number of tuneable hyperparameters which may help us. In this project, we focus just on `n_trees` and `max_depth` due to computational and time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# measure performance of gbm using different values for ntrees and max_depth\n",
    "def param_search(ntree_vals, md_vals, X_train, y_train, resample = True):\n",
    "    scores = np.zeros((len(ntree_vals), len(md_vals)))\n",
    "    print \"Running 7-fold cross validation for:\"\n",
    "    for i in range(len(ntree_vals)):\n",
    "        for j in range(len(md_vals)):\n",
    "            print \"n_trees = %d,\" % ntree_vals[i],\n",
    "            print \"max_depth = %d,\" % md_vals[j]\n",
    "            evalgbm = EvalGBM(X_train, y_train, n_estimators = ntree_vals[i], max_depth = md_vals[j], metric = 'roc')\n",
    "            scores[i,j] = np.mean(evalgbm.kfold(7, False, False, resample).Test_score)\n",
    "\n",
    "    print scores\n",
    "    score_opt = scores.max()\n",
    "    params_opt = np.where(scores==score_opt)\n",
    "    \n",
    "    return (score_opt, params_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform 2D grid search over a subset of hyperparameter values\n",
    "ntree_vals = (20,40,60,80)\n",
    "md_vals = (1,2,3,4)\n",
    "param_search(ntree_vals, md_vals, X_train, y_train, resample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preliminary code to start looking at feature selection\n",
    "evalgbm = EvalGBM(X_train, y_train, n_estimators = 20, max_depth = 2)\n",
    "rXtrain, rytrain = evalgbm.resample(X_train, y_train, False)\n",
    "evalgbm.gbm.fit(rXtrain, rytrain)\n",
    "features = list(X_train.columns.values)\n",
    "print features[np.argmax(evalgbm.gbm.feature_importances_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Based on the tuning results above, a model of 40 estimators with a max-depth of two should ouperform our basic model. Here, we train and test that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalgbm = EvalGBM(X_train, y_train, n_estimators = 40, max_depth = 2)\n",
    "rXtrain, rytrain = evalgbm.resample(X_train, y_train, False)\n",
    "rXtest, rytest = evalgbm.resample(X_test, y_test, False)\n",
    "evalgbm.setTrain(rXtrain, np.asarray(rytrain))\n",
    "evalgbm.setTest(rXtest, rytest)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, f1_score, confusion_matrix, roc_auc_score, accuracy_score\n",
    "# fit to train data\n",
    "evalgbm.gbm.fit(rXtrain, rytrain)\n",
    "\n",
    "test_probs = evalgbm.gbm.predict_proba(rXtest)[:,1]\n",
    "train_probs = evalgbm.gbm.predict_proba(rXtrain)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(rytrain, train_probs, pos_label = 1)\n",
    "f1s = [f1_score(rytrain, (train_probs>t).astype(int), average = 'binary') for t in thresholds]\n",
    "f_i = np.argmax(np.asarray(f1s))\n",
    "\n",
    "test_probs_f = (test_probs>thresholds[f_i]).astype(int)\n",
    "print \"\\nOptimum threshold to maximize f1:\", thresholds[f_i]\n",
    "trainscore = f1_score(rytrain, (train_probs>thresholds[f_i]).astype(int), average = 'binary')\n",
    "testscore = f1_score(rytest, test_probs_f, average = 'binary')\n",
    "\n",
    "print \"Train Score: %f Test Score: %f\" % (trainscore, testscore)\n",
    "\n",
    "print \"AU-ROC:\", roc_auc_score(rytest, test_probs)\n",
    "print \"Accuracy:\", accuracy_score(rytest, test_probs_f)\n",
    "print \"Confusion Matrix:\\n\", confusion_matrix(rytest, test_probs_f)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(rytest, test_probs, pos_label = 1)\n",
    "plt.plot(fpr, tpr, color = 'darkorange', lw = lw, label = 'ROC curve')\n",
    "plt.plot([0,1], [0,1], color = 'navy', lw = lw, linestyle = '--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "plt.figure(figsize = (10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(fpr)\n",
    "print list(tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Perhaps we can combine the LR and GBM classifiers to achieve even better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import EvalLR\n",
    "import class_EvalLR\n",
    "reload(class_EvalLR)\n",
    "from class_EvalLR import EvalLR\n",
    "\n",
    "# train our best LR classifier\n",
    "evalr = EvalLR(X_train, y_train, c=0.125)\n",
    "evalr.lr.fit(rXtrain, rytrain)\n",
    "test_probs_lr = evalr.lr.predict_proba(rXtest)[:,1]\n",
    "train_probs_lr = evalr.lr.predict_proba(rXtrain)[:,1]\n",
    "\n",
    "# average predictions for both classifiers\n",
    "test_probs_bg = (test_probs_lr + test_probs)/2\n",
    "train_probs_bg = (train_probs_lr + train_probs)/2\n",
    "\n",
    "# evaluate bagged classifier\n",
    "fpr, tpr, thresholds = roc_curve(rytrain, train_probs_bg, pos_label = 1)\n",
    "f1s = [f1_score(rytrain, (train_probs_bg>t).astype(int), average = 'binary') for t in thresholds]\n",
    "f_i = np.argmax(np.asarray(f1s))\n",
    "\n",
    "test_probs_f = (test_probs_bg>thresholds[f_i]).astype(int)\n",
    "print \"\\nOptimum threshold to maximize f1:\", thresholds[f_i]\n",
    "trainscore = f1_score(rytrain, (train_probs_bg>thresholds[f_i]).astype(int), average = 'binary')\n",
    "testscore = f1_score(rytest, test_probs_f, average = 'binary')\n",
    "\n",
    "print \"Train Score: %f Test Score: %f\" % (trainscore, testscore)\n",
    "\n",
    "print \"AU-ROC:\", roc_auc_score(rytest, test_probs_bg)\n",
    "print \"Accuracy:\", accuracy_score(rytest, test_probs_f)\n",
    "print \"Confusion Matrix:\\n\", confusion_matrix(rytest, test_probs_f)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(rytest, test_probs_bg, pos_label = 1)\n",
    "plt.plot(fpr, tpr, color = 'darkorange', lw = lw, label = 'ROC curve')\n",
    "plt.plot([0,1], [0,1], color = 'navy', lw = lw, linestyle = '--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "plt.figure(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(fpr)\n",
    "print list(tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc_subjects = list(set(SSC_asd.index.values.tolist() + SSC_ctrl.index.values.tolist()) & set(subjects))\n",
    "agre_subjects = list(set(Agre_asd.index.values.tolist() + Agre_ctrl.index.values.tolist()) & set(subjects))\n",
    "\n",
    "ssc_test_subjects = list(set(ssc_subjects) & set(rytest.index.values.tolist()))\n",
    "agre_test_subjects = list(set(agre_subjects) & set(rytest.index.values.tolist()))\n",
    "\n",
    "print len(scc_subjects)\n",
    "print len(agre_subjects)\n",
    "\n",
    "ssc_ytest = rytest.ix[ssc_test_subjects]\n",
    "ssc_Xtest = rXtest.ix[ssc_test_subjects]\n",
    "\n",
    "agre_ytest = rytest.ix[agre_test_subjects]\n",
    "agre_Xtest = rXtest.ix[agre_test_subjects]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check, measure performance separately on the SSC and Agre members of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# results for ssc\n",
    "test_probs_lr = evalr.lr.predict_proba(ssc_Xtest)[:,1]\n",
    "test_probs = evalgbm.gbm.predict_proba(ssc_Xtest)[:,1]\n",
    "\n",
    "# average predictions for both classifiers\n",
    "test_probs_bg = (test_probs_lr + test_probs)/2\n",
    "\n",
    "print \"AU-ROC:\", roc_auc_score(ssc_ytest, test_probs_bg)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(ssc_ytest, test_probs_bg, pos_label = 1)\n",
    "plt.plot(fpr, tpr, color = 'darkorange', lw = lw, label = 'ROC curve')\n",
    "plt.plot([0,1], [0,1], color = 'navy', lw = lw, linestyle = '--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "plt.figure(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# results for agre\n",
    "test_probs_lr = evalr.lr.predict_proba(agre_Xtest)[:,1]\n",
    "test_probs = evalgbm.gbm.predict_proba(agre_Xtest)[:,1]\n",
    "\n",
    "# average predictions for both classifiers\n",
    "test_probs_bg = (test_probs_lr + test_probs)/2\n",
    "\n",
    "print \"AU-ROC:\", roc_auc_score(agre_ytest, test_probs_bg)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(agre_ytest, test_probs_bg, pos_label = 1)\n",
    "plt.plot(fpr, tpr, color = 'darkorange', lw = lw, label = 'ROC curve')\n",
    "plt.plot([0,1], [0,1], color = 'navy', lw = lw, linestyle = '--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "plt.figure(figsize = (10,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
